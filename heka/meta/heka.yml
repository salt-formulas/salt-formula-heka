{%- from "heka/map.jinja" import log_collector with context %}
{%- from "heka/map.jinja" import metric_collector with context %}
{%- from "heka/map.jinja" import remote_collector with context %}
{%- from "heka/map.jinja" import aggregator  with context %}

log_collector:
  filter:
    aggregated_http_metrics:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/http_metrics_aggregator.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      message_matcher: "Type == 'log' && Fields[http_response_time] != NIL"
      ticker_interval: 10
      config:
        hostname: '{{ grains.host }}'
        interval: 10
        max_timer_inject: 10
        bulk_size: 523
        percentile: 90
        grace_time: 5
    log_counter:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/logs_counter.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: true
      message_matcher: "Type == 'log' && Logger =~ /^openstack\\\\./"
      ticker_interval: 1
      config:
        hostname: '{{ grains.host }}'
        interval: 60
        grace_interval: 30
{%- if log_collector.elasticsearch_host is defined %}
  encoder:
    elasticsearch:
      engine: elasticsearch
{%- endif %}
  output:
    metric_collector:
      engine: tcp
      host: 127.0.0.1
      port: 5567
      message_matcher: "(Type == 'metric' || Type == 'heka.sandbox.metric' || Type == 'heka.sandbox.bulk_metric')"
    log_dashboard:
      engine: dashboard
      host: 127.0.0.1
      port: 4352
      ticker_interval: 30
{%- if log_collector.elasticsearch_host is defined %}
    elasticsearch:
      engine: elasticsearch
      server: "http://{{ log_collector.elasticsearch_host }}:{{ log_collector.elasticsearch_port }}"
      encoder: elasticsearch_encoder
      message_matcher: "Type == 'log'"
{%- endif %}
metric_collector:
  decoder:
    collectd:
      engine: sandbox
      module_file: /usr/share/lma_collector/decoders/collectd.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      config:
        hostname: '{{ grains.fqdn.split('.')[0] }}'
        swap_size: {{ salt['ps.swap_memory']()['total'] }}
    metric:
      engine: sandbox
      module_file: /usr/share/lma_collector/decoders/metric.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      config:
        deserialize_bulk_metric_for_loggers: 'aggregated_http_metrics_filter hdd_errors_counter_filter'
  input:
    heka_collectd:
      engine: http
      address: 127.0.0.1
      port: 8325
      decoder: collectd_decoder
      splitter: NullSplitter
    heka_metric:
      engine: tcp
      address: 0.0.0.0
      port: 5567
      decoder: metric_decoder
      splitter: HekaFramingSplitter
  filter:
    heka_metric_collector:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/heka_monitoring.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: false
      message_matcher: "Type == 'heka.all-report'"
{%- if metric_collector.influxdb_host is defined %}
    influxdb_accumulator:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/influxdb_accumulator.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: false
      message_matcher: "Type =~ /metric$/"
      ticker_interval: 1
      config:
        tag_fields: "deployment_id environment_label tenant_id user_id"
        time_precision: "{{ metric_collector.influxdb_time_precision }}"
{%- endif %}
{%- if metric_collector.influxdb_host is defined %}
  encoder:
    influxdb:
      engine: payload
      append_newlines: false
      prefix_ts: false
{%- endif %}
{%- if metric_collector.nagios_host is defined %}
    nagios:
      engine: sandbox
      module_file: /usr/share/lma_collector/encoders/status_nagios.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
{%- endif %}
  output:
    metric_dashboard:
      engine: dashboard
      host: 127.0.0.1
      port: 4353
      ticker_interval: 30
{%- if metric_collector.influxdb_host is defined %}
    influxdb:
      engine: http
      address: "http://{{ metric_collector.influxdb_host }}:{{ metric_collector.influxdb_port }}/write?db={{ metric_collector.influxdb_database }}&precision={{ metric_collector.influxdb_time_precision }}"
    {%- if metric_collector.influxdb_username and metric_collector.influxdb_password %}
      username: "{{ metric_collector.influxdb_username }}"
      password: "{{ metric_collector.influxdb_password }}"
    {%- endif %}
      message_matcher: "Fields[payload_type] == 'txt' && Fields[payload_name] == 'influxdb'"
      encoder: influxdb_encoder
      timeout: {{ metric_collector.influxdb_timeout }}
{%- endif %}
{%- if metric_collector.aggregator_host is defined %}
    aggregator:
      engine: tcp
      host: "{{ metric_collector.aggregator_host }}"
      port: "{{ metric_collector.aggregator_port }}"
      message_matcher: "Type == 'heka.sandbox.afd_metric'"
{%- endif %}
{%- if metric_collector.nagios_host is defined %}
    nagios_alarm:
      engine: http
      address: "http://{{ metric_collector.nagios_host }}:{{metric_collector.nagios_port }}/status"
      message_matcher: "Type == 'heka.sandbox.afd_metric' && Fields[no_alerting] == NIL"
      encoder: nagios_encoder
      {%- if metric_collector.nagios_username is defined and metric_collector.nagios_password is defined %}
      username: {{ metric_collector.get('nagios_username') }}
      password: {{ metric_collector.get('nagios_password') }}
      {%- endif %}
      max_buffer_size: 1048576
      max_file_size: 524288
      full_action: drop
{%- endif %}
remote_collector:
  decoder:
    collectd:
      engine: sandbox
      module_file: /usr/share/lma_collector/decoders/collectd.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      config:
        hostname: '{{ grains.host }}'
{%- if remote_collector.amqp_host is defined %}
    notification:
      engine: sandbox
      module_file: /usr/share/lma_collector/decoders/notification.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      config:
        include_full_notification: false
{%- endif %}
  input:
    heka_collectd:
      engine: http
      address: 0.0.0.0
      port: 8326
      decoder: collectd_decoder
      splitter: NullSplitter
{%- if remote_collector.amqp_host is defined %}
{%- for notification_level in ('info', 'warn', 'error') %}
    amqp_notification_{{ notification_level }}:
      engine: amqp
      host: {{ remote_collector.amqp_host }}
      port: {{ remote_collector.amqp_port }}
      user: {{ remote_collector.amqp_user }}
      password: {{ remote_collector.amqp_password }}
      vhost: {{ remote_collector.get('amqp_vhost', '') }}
      exchange: {{ remote_collector.get('amqp_exchange', 'nova') }}
      exchange_type: topic
      exchange_durability: false
      exchange_auto_delete: false
      queue_auto_delete: false
      queue: "{{ remote_collector.amqp_notification_topic }}.{{ notification_level }}"
      routing_key: "{{ remote_collector.amqp_notification_topic }}.{{ notification_level }}"
      can_exit: false
      decoder: notification_decoder
{%- endfor %}
{%- endif %}
{%- if remote_collector.influxdb_host is defined %}
  filter:
    influxdb_accumulator:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/influxdb_accumulator.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: false
      message_matcher: "Type =~ /metric$/"
      ticker_interval: 1
      config:
        tag_fields: "deployment_id environment_label tenant_id user_id"
        time_precision: "{{ remote_collector.influxdb_time_precision }}"
{%- endif %}
{%- if remote_collector.influxdb_host is defined or remote_collector.elasticsearch_host is defined %}
  encoder:
{%- if remote_collector.influxdb_host is defined %}
    influxdb:
      engine: payload
      append_newlines: false
      prefix_ts: false
{%- endif %}
{%- if remote_collector.elasticsearch_host is defined %}
    elasticsearch:
      engine: elasticsearch
{%- endif %}
{%- endif %}
  output:
    remote_collector_dashboard:
      engine: dashboard
      host: 127.0.0.1
      port: 4354
      ticker_interval: 30
{%- if remote_collector.influxdb_host is defined %}
    influxdb:
      engine: http
      address: "http://{{ remote_collector.influxdb_host }}:{{ remote_collector.influxdb_port }}/write?db={{ remote_collector.influxdb_database }}&precision={{ remote_collector.influxdb_time_precision }}"
    {%- if remote_collector.influxdb_username and remote_collector.influxdb_password %}
      username: "{{ remote_collector.influxdb_username }}"
      password: "{{ remote_collector.influxdb_password }}"
    {%- endif %}
      message_matcher: "Fields[payload_type] == 'txt' && Fields[payload_name] == 'influxdb'"
      encoder: influxdb_encoder
      timeout: {{ remote_collector.influxdb_timeout }}
{%- endif %}
{%- if remote_collector.aggregator_host is defined %}
    aggregator:
      engine: tcp
      host: "{{ remote_collector.aggregator_host }}"
      port: "{{ remote_collector.aggregator_port }}"
      message_matcher: "Type == 'heka.sandbox.afd_metric'"
{%- endif %}
{%- if remote_collector.elasticsearch_host is defined %}
    elasticsearch:
      engine: elasticsearch
      server: "http://{{ remote_collector.elasticsearch_host }}:{{ remote_collector.elasticsearch_port }}"
      encoder: elasticsearch_encoder
      message_matcher: "Type == 'notification'"
{%- endif %}
aggregator:
  policy:
    # A policy defining that the cluster's status depends on the member with
    # the highest severity, typically used for a cluster of services.
    highest_severity:
    - status: down
      trigger:
        logical_operator: or
        rules:
        - function: count
          arguments: [ down ]
          relational_operator: '>'
          threshold: 0
    - status: critical
      trigger:
        logical_operator: or
        rules:
        - function: count
          arguments: [ critical ]
          relational_operator: '>'
          threshold: 0
    - status: warning
      trigger:
        logical_operator: or
        rules:
        - function: count
          arguments: [ warning ]
          relational_operator: '>'
          threshold: 0
    - status: okay
      trigger:
        logical_operator: or
        rules:
        - function: count
          arguments: [ okay ]
          relational_operator: '>'
          threshold: 0
    - status: unknown
    # A policy which is typically used for clusters managed by Pacemaker
    # with the no-quorum-policy set to 'stop'.
    majority_of_members:
    - status: down
      trigger:
        logical_operator: or
        rules:
        - function: percent
          arguments: [ down ]
          relational_operator: '>'
          threshold: 50
    - status: critical
      trigger:
        logical_operator: and
        rules:
        - function: percent
          arguments: [ down, critical ]
          relational_operator: '>'
          threshold: 20
        - function: percent
          arguments: [ okay ]
          relational_operator: '<'
          threshold: 50
          function: percent
    - status: warning
      trigger:
        logical_operator: or
        rules:
        - function: percent
          arguments: [ okay ]
          relational_operator: '<'
          threshold: 50
          function: percent
    - status: okay
    # A policy which is typically used for stateless clusters
    availability_of_members:
    - status: down
      trigger:
        logical_operator: or
        rules:
        - function: count
          arguments: [ okay ]
          relational_operator: '=='
          threshold: 0
    - status: critical
      trigger:
        logical_operator: and
        rules:
        - function: count
          arguments: [ okay ]
          relational_operator: '=='
          threshold: 1
        - function: count
          arguments: [ critical, down ]
          relational_operator: '>'
          threshold: 1
    - status: warning
      trigger:
        logical_operator: or
        rules:
        - function: percent
          arguments: [ okay ]
          relational_operator: '<'
          threshold: 100
    - status: okay
      trigger:
        logical_operator: or
        rules:
        - function: percent
          arguments: [ okay ]
          relational_operator: '=='
          threshold: 100
    - status: unknown
  input:
    heka_metric:
      engine: tcp
      address: 0.0.0.0
      port: 5565
      decoder: ProtobufDecoder
      splitter: HekaFramingSplitter
{%- if aggregator.influxdb_host is defined %}
  filter:
    influxdb_accumulator:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/influxdb_accumulator.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: false
      message_matcher: "Type == 'heka.sandbox.gse_metric' || Type == 'heka.sandbox.metric'"
      ticker_interval: 1
      config:
        tag_fields: "deployment_id environment_label tenant_id user_id"
        time_precision: "{{ aggregator.influxdb_time_precision }}"
    influxdb_annotation:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/influxdb_annotation.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: false
      message_matcher: "Type == 'heka.sandbox.gse_metric'"
{%- endif %}
  encoder:
{%- if aggregator.influxdb_host is defined %}
    influxdb:
      engine: payload
      append_newlines: false
      prefix_ts: false
{%- endif %}
{%- if aggregator.nagios_host is defined %}
    nagios:
      engine: sandbox
      module_file: /usr/share/lma_collector/encoders/status_nagios.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      config:
        default_nagios_host: "{{ aggregator.nagios_default_host_alarm_clusters }}"
        {%- if aggregator.nagios_host_dimension_key is defined %}
        nagios_host_dimension_key: "{{ aggregator.nagios_host_dimension_key }}"
        {%- endif %}
{%- endif %}
  output:
{%- if aggregator.influxdb_host is defined %}
    influxdb:
      engine: http
      address: "http://{{ aggregator.influxdb_host }}:{{ aggregator.influxdb_port }}/write?db={{ aggregator.influxdb_database }}&precision={{ aggregator.influxdb_time_precision }}"
    {%- if aggregator.influxdb_username and aggregator.influxdb_password %}
      username: "{{ aggregator.influxdb_username }}"
      password: "{{ aggregator.influxdb_password }}"
    {%- endif %}
      message_matcher: "Fields[payload_type] == 'txt' && Fields[payload_name] == 'influxdb'"
      encoder: influxdb_encoder
      timeout: {{ aggregator.influxdb_timeout }}
{%- endif %}
{%- if aggregator.nagios_host is defined %}
    nagios_alarm_cluster:
      engine: http
      address: "http://{{ aggregator.nagios_host }}:{{aggregator.nagios_port }}/status"
      message_matcher: "Type == 'heka.sandbox.gse_metric' && Fields[no_alerting] == NIL"
      encoder: nagios_encoder
      {%- if aggregator.nagios_username is defined and aggregator.nagios_password is defined %}
      username: {{ aggregator.get('nagios_username') }}
      password: {{ aggregator.get('nagios_password') }}
      {%- endif %}
      max_buffer_size: 1048576
      max_file_size: 524288
      full_action: drop
{%- endif %}
