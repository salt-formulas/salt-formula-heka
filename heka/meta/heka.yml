{%- from "heka/map.jinja" import log_collector with context %}
{%- from "heka/map.jinja" import metric_collector with context %}
{%- from "heka/map.jinja" import remote_collector with context %}
{%- from "heka/map.jinja" import aggregator  with context %}

log_collector:
  filter:
    aggregated_http_metrics:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/http_metrics_aggregator.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      message_matcher: "Type == 'log' && Fields[http_response_time] != NIL"
      ticker_interval: 10
      config:
        hostname: '{{ pillar.get('linux',{}).get('system',{}).name|default(grains.host) }}'
        interval: 10
        max_timer_inject: 10
        bulk_size: 523
        percentile: 90
        grace_time: 5
    log_counter:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/logs_counter.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: true
      message_matcher: "Type == 'log' && Logger =~ /^openstack\\\\./"
      ticker_interval: 1
      config:
        hostname: '{{ pillar.get('linux',{}).get('system',{}).name|default(grains.host) }}'
        interval: 60
        grace_interval: 30
{%- if log_collector.elasticsearch_host is defined %}
  encoder:
    elasticsearch:
      engine: elasticsearch
{%- endif %}
  output:
    metric_collector:
      engine: tcp
      host: 127.0.0.1
      port: 5567
      message_matcher: "(Type == 'metric' || Type == 'heka.sandbox.metric' || Type == 'heka.sandbox.bulk_metric')"
    log_dashboard:
      engine: dashboard
      host: 127.0.0.1
      port: 4352
      ticker_interval: 30
{%- if log_collector.elasticsearch_host is defined %}
    elasticsearch:
      engine: elasticsearch
      server: "http://{{ log_collector.elasticsearch_host }}:{{ log_collector.elasticsearch_port }}"
      encoder: elasticsearch_encoder
      message_matcher: "Type == 'log' || Type == 'notification'"
{%- endif %}
metric_collector:
  decoder:
    collectd:
      engine: sandbox
      module_file: /usr/share/lma_collector/decoders/collectd.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      config:
        hostname: '{{ pillar.get('linux',{}).get('system',{}).name|default(grains.host) }}'
        swap_size: 4294967296
    metric:
      engine: sandbox
      module_file: /usr/share/lma_collector/decoders/metric.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      config:
        deserialize_bulk_metric_for_loggers: 'aggregated_http_metrics_filter hdd_errors_counter_filter'
  input:
    heka_collectd:
      engine: http
      address: 127.0.0.1
      port: 8325
      decoder: collectd_decoder
      splitter: NullSplitter
    heka_metric:
      engine: tcp
      address: 0.0.0.0
      port: 5567
      decoder: metric_decoder
      splitter: HekaFramingSplitter
  filter:
    heka_metric_collector:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/heka_monitoring.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: false
      message_matcher: "Type == 'heka.all-report'"
{%- if metric_collector.influxdb_host is defined %}
    influxdb_accumulator:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/influxdb_accumulator.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: false
      message_matcher: "Fields[aggregator] == NIL && Type =~ /metric$/"
      ticker_interval: 1
      config:
        tag_fields: "deployment_id environment_label tenant_id user_id"
        time_precision: "{{ metric_collector.influxdb_time_precision }}"
{%- endif %}
{%- if metric_collector.influxdb_host is defined %}
  encoder:
    influxdb:
      engine: payload
      append_newlines: false
      prefix_ts: false
{%- endif %}
  output:
    metric_dashboard:
      engine: dashboard
      host: 127.0.0.1
      port: 4353
      ticker_interval: 30
{%- if metric_collector.influxdb_host is defined %}
    influxdb:
      engine: http
      address: "http://{{ metric_collector.influxdb_host }}:{{ metric_collector.influxdb_port }}/write?db={{ metric_collector.influxdb_database }}&precision={{ metric_collector.influxdb_time_precision }}"
    {%- if metric_collector.influxdb_username and metric_collector.influxdb_password %}
      username: "{{ metric_collector.influxdb_username }}"
      password: "{{ metric_collector.influxdb_password }}"
    {%- endif %}
      message_matcher: "Fields[payload_type] == 'txt' && Fields[payload_name] == 'influxdb'"
      encoder: influxdb_encoder
      timeout: {{ metric_collector.influxdb_timeout }}
{%- endif %}
{%- if metric_collector.aggregator_host is defined %}
    aggregator:
      engine: tcp
      host: "{{ metric_collector.aggregator_host }}"
      port: "{{ metric_collector.aggregator_port }}"
      message_matcher: "Fields[aggregator] == NIL && Type == 'heka.sandbox.afd_metric'"
{%- endif %}
remote_collector:
  decoder:
    collectd:
      engine: sandbox
      module_file: /usr/share/lma_collector/decoders/collectd.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      config:
        hostname: '{{ pillar.get('linux',{}).get('system',{}).name|default(grains.host) }}'
  input:
    heka_collectd:
      engine: http
      address: 127.0.0.1
      port: 8326
      decoder: collectd_decoder
      splitter: NullSplitter
{%- if remote_collector.influxdb_host is defined %}
  filter:
    influxdb_accumulator:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/influxdb_accumulator.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: false
      message_matcher: "Type == 'heka.sandbox.afd_metric'"
      ticker_interval: 1
      config:
        tag_fields: "deployment_id environment_label tenant_id user_id"
        time_precision: "{{ remote_collector.influxdb_time_precision }}"
{%- endif %}
{%- if remote_collector.influxdb_host is defined %}
  encoder:
    influxdb:
      engine: payload
      append_newlines: false
      prefix_ts: false
{%- endif %}
  output:
    remote_collector_dashboard:
      engine: dashboard
      host: 127.0.0.1
      port: 4354
      ticker_interval: 30
{%- if remote_collector.influxdb_host is defined %}
    influxdb:
      engine: http
      address: "http://{{ remote_collector.influxdb_host }}:{{ remote_collector.influxdb_port }}/write?db={{ remote_collector.influxdb_database }}&precision={{ remote_collector.influxdb_time_precision }}"
    {%- if remote_collector.influxdb_username and remote_collector.influxdb_password %}
      username: "{{ remote_collector.influxdb_username }}"
      password: "{{ remote_collector.influxdb_password }}"
    {%- endif %}
      message_matcher: "Fields[payload_type] == 'txt' && Fields[payload_name] == 'influxdb'"
      encoder: influxdb_encoder
      timeout: {{ remote_collector.influxdb_timeout }}
{%- endif %}
{%- if remote_collector.aggregator_host is defined %}
    aggregator:
      engine: tcp
      host: "{{ remote_collector.aggregator_host }}"
      port: "{{ remote_collector.aggregator_port }}"
      message_matcher: "Fields[aggregator] == NIL && Type == 'heka.sandbox.afd_metric'"
{%- endif %}
aggregator:
  policy:
    # A policy defining that the cluster's status depends on the member with
    # the highest severity, typically used for a cluster of services.
    highest_severity:
    - status: down
      trigger:
        logical_operator: or
        rules:
        - function: count
          arguments: [ down ]
          relational_operator: '>'
          threshold: 0
    - status: critical
      trigger:
        logical_operator: or
        rules:
        - function: count
          arguments: [ critical ]
          relational_operator: '>'
          threshold: 0
    - status: warning
      trigger:
        logical_operator: or
        rules:
        - function: count
          arguments: [ warning ]
          relational_operator: '>'
          threshold: 0
    - status: okay
      trigger:
        logical_operator: or
        rules:
        - function: count
          arguments: [ okay ]
          relational_operator: '>'
          threshold: 0
    - status: unknown
    # A policy which is typically used for clusters managed by Pacemaker
    # with the no-quorum-policy set to 'stop'.
    majority_of_members:
    - status: down
      trigger:
        logical_operator: or
        rules:
        - function: percent
          arguments: [ down ]
          relational_operator: '>'
          threshold: 50
    - status: critical
      trigger:
        logical_operator: and
        rules:
        - function: percent
          arguments: [ down, critical ]
          relational_operator: '>'
          threshold: 20
        - function: percent
          arguments: [ okay ]
          relational_operator: '<'
          threshold: 50
          function: percent
    - status: warning
      trigger:
        logical_operator: or
        rules:
        - function: percent
          arguments: [ okay ]
          relational_operator: '<'
          threshold: 50
          function: percent
    - status: okay
    # A policy which is typically used for stateless clusters
    availability_of_members:
    - status: down
      trigger:
        logical_operator: or
        rules:
        - function: count
          arguments: [ okay ]
          relational_operator: '=='
          threshold: 0
    - status: critical
      trigger:
        logical_operator: and
        rules:
        - function: count
          arguments: [ okay ]
          relational_operator: '=='
          threshold: 1
        - function: count
          arguments: [ critical, down ]
          relational_operator: '>'
          threshold: 1
    - status: warning
      trigger:
        logical_operator: or
        rules:
        - function: percent
          arguments: [ okay ]
          relational_operator: '<'
          threshold: 100
    - status: okay
      trigger:
        logical_operator: or
        rules:
        - function: percent
          arguments: [ okay ]
          relational_operator: '=='
          threshold: 100
    - status: unknown
  input:
    heka_metric:
      engine: tcp
      address: 0.0.0.0
      port: 5565
      decoder: ProtobufDecoder
      splitter: HekaFramingSplitter
{%- if aggregator.influxdb_host is defined %}
  filter:
    influxdb_accumulator:
      engine: sandbox
      module_file: /usr/share/lma_collector/filters/influxdb_accumulator.lua
      module_dir: /usr/share/lma_collector/common;/usr/share/heka/lua_modules
      preserve_data: false
      message_matcher: "Type == 'heka.sandbox.gse_metric'"
      ticker_interval: 1
      config:
        tag_fields: "deployment_id environment_label tenant_id user_id"
        time_precision: "{{ aggregator.influxdb_time_precision }}"
{%- endif %}
{%- if aggregator.influxdb_host is defined %}
  encoder:
    influxdb:
      engine: payload
      append_newlines: false
      prefix_ts: false
{%- endif %}
{%- if aggregator.influxdb_host is defined %}
  output:
    influxdb:
      engine: http
      address: "http://{{ aggregator.influxdb_host }}:{{ aggregator.influxdb_port }}/write?db={{ aggregator.influxdb_database }}&precision={{ aggregator.influxdb_time_precision }}"
    {%- if aggregator.influxdb_username and aggregator.influxdb_password %}
      username: "{{ aggregator.influxdb_username }}"
      password: "{{ aggregator.influxdb_password }}"
    {%- endif %}
      message_matcher: "Fields[payload_type] == 'txt' && Fields[payload_name] == 'influxdb'"
      encoder: influxdb_encoder
      timeout: {{ aggregator.influxdb_timeout }}
{%- endif %}
